{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from statistics import median\n",
    "\n",
    "#from dee.event_types import get_event_template\n",
    "\n",
    "\n",
    "def load_line_json_iterator(filepath):\n",
    "    with open(filepath, \"rt\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            d = json.loads(line.strip())\n",
    "            yield d\n",
    "\n",
    "\n",
    "def load_json(filepath):\n",
    "    with open(filepath, \"rt\", encoding=\"utf-8\") as fin:\n",
    "        return json.load(fin)\n",
    "\n",
    "\n",
    "def sent_seg(\n",
    "    text,\n",
    "    special_seg_indicators=None,\n",
    "    lang=\"zh\",\n",
    "    punctuations=None,\n",
    "    quotation_seg_mode=True,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    cut texts into sentences (in chinese language).\n",
    "    Args:\n",
    "        text <str>: texts ready to be cut\n",
    "        special_seg_indicators <list>: some special segment indicators and\n",
    "            their replacement ( [indicator, replacement] ), in baike data,\n",
    "            this argument could be `[('###', '\\n'), ('%%%', ' '), ('%%', ' ')]`\n",
    "        lang <str>: languages that your corpus is, support `zh` for Chinese\n",
    "            and `en` for English now.\n",
    "        punctuations <set>: you can split the texts by specified punctuations.\n",
    "            texts will not be splited by `;`, so you can specify them by your own.\n",
    "        quotation_seg_mode <bool>: if True, the quotations will be regarded as a\n",
    "            part of the former sentence.\n",
    "            e.g. `我说：“翠花，上酸菜。”，她说：“欸，好嘞。”`\n",
    "            the text will be splited into\n",
    "            ['我说：“翠花，上酸菜。”，', '她说：“欸，好嘞。”'], other than\n",
    "            ['我说：“翠花，上酸菜。', '”，她说：“欸，好嘞。”']\n",
    "    Rrturns:\n",
    "        <list>: a list of strings, which are splited sentences.\n",
    "    \"\"\"\n",
    "    # if texts are not in string format, raise an error\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError\n",
    "\n",
    "    # if the text is empty, return a list with an empty string\n",
    "    if len(text) == 0:\n",
    "        return []\n",
    "\n",
    "    text_return = text\n",
    "\n",
    "    # segment on specified indicators\n",
    "    # special indicators standard, like [('###', '\\n'), ('%%%', '\\t'), ('\\s', '')]\n",
    "    if special_seg_indicators:\n",
    "        for indicator in special_seg_indicators:\n",
    "            text_return = re.sub(indicator[0], indicator[1], text_return)\n",
    "\n",
    "    if lang == \"zh\":\n",
    "        punkt = {\"。\", \"？\", \"！\", \"…\"}\n",
    "    elif lang == \"en\":\n",
    "        punkt = {\".\", \"?\", \"!\"}\n",
    "    if punctuations:\n",
    "        punkt = punkt | punctuations\n",
    "\n",
    "    if quotation_seg_mode:\n",
    "        text_return = re.sub(\n",
    "            \"([%s]+[’”`'\\\"]*)\" % (\"\".join(punkt)), \"\\\\1\\n\", text_return\n",
    "        )\n",
    "    else:\n",
    "        text_return = re.sub(\"([{}])\".format(\"\".join(punkt)), \"\\\\1\\n\", text_return)\n",
    "\n",
    "    # drop sentences with no length\n",
    "    return [\n",
    "        s.strip()\n",
    "        for s in filter(\n",
    "            lambda x: len(x.strip()) == 1\n",
    "            and x.strip() not in punkt\n",
    "            or len(x.strip()) > 0,\n",
    "            text_return.split(\"\\n\"),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "def stat_sent_len(filepath):\n",
    "    num_sents = []\n",
    "    sent_len = []\n",
    "    for d in load_line_json_iterator(filepath):\n",
    "        sents = sent_seg(d[\"text\"])\n",
    "        num_sents.append(len(sents))\n",
    "        lens = [len(sent) for sent in sents]\n",
    "        sent_len.extend(lens)\n",
    "        # if min(lens) < 5:\n",
    "        #     print(\"================= raw text =================\")\n",
    "        #     print(d[\"text\"])\n",
    "        #     print(\"================= processed text =================\")\n",
    "        #     print(\"\\n\".join(filter(lambda x: len(x) < 5, sents)))\n",
    "        #     breakpoint()\n",
    "    sent_len_counter = Counter(sent_len)\n",
    "    print(\n",
    "        (\n",
    "            f\"num_sents: min: {min(num_sents)}, median: {median(num_sents)}, max: {max(num_sents)}\\n\"\n",
    "            f\"sent_len: min: {min(sent_len)}, median: {median(sent_len)}, max: {max(sent_len)}\"\n",
    "            f\"{sent_len_counter.most_common()}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# qy: get ranges of a given word \"span\"\n",
    "def get_span_drange(sents, span):\n",
    "    drange = []\n",
    "    common_span = (\n",
    "        span.replace(\"*\", \"\\*\")\n",
    "        .replace(\"?\", \"\\?\")\n",
    "        .replace(\"+\", \"\\+\")\n",
    "        .replace(\"[\", \"\\[\")\n",
    "        .replace(\"]\", \"\\]\")\n",
    "        .replace(\"(\", \"\\(\")\n",
    "        .replace(\")\", \"\\)\")\n",
    "        .replace(\".\", \"\\.\")\n",
    "        .replace(\"-\", \"\\-\")\n",
    "    )  # noqa: W605\n",
    "    for sent_idx, sent in enumerate(sents):\n",
    "        # qy: word to be found shorter than the sentence\n",
    "        if len(sent) < len(common_span):\n",
    "            continue\n",
    "        for ocurr in re.finditer(common_span, sent):\n",
    "            span_pos = ocurr.span()\n",
    "            if (\n",
    "                (\n",
    "                    \"0\" <= span[0] <= \"9\"\n",
    "                    and \"0\" <= sents[sent_idx][span_pos[0] - 1] <= \"9\"\n",
    "                    and span_pos[0] - 1 > -1\n",
    "                )\n",
    "                or (\n",
    "                    \"0\" <= span[0] <= \"9\"\n",
    "                    and \"0\" <= sents[sent_idx][span_pos[0] - 2]\n",
    "                    and sents[sent_idx][span_pos[0] - 1] == \".\"\n",
    "                    and span_pos[0] - 2 > -1\n",
    "                )\n",
    "                or (\n",
    "                    \"0\" <= span[-1] <= \"9\"\n",
    "                    and span_pos[1] < len(sents[sent_idx])\n",
    "                    and \"0\" <= sents[sent_idx][span_pos[1]] <= \"9\"\n",
    "                )\n",
    "                or (\n",
    "                    \"0\" <= span[-1] <= \"9\"\n",
    "                    and span_pos[1] + 1 < len(sents[sent_idx])\n",
    "                    and sents[sent_idx][span_pos[1]] == \".\"\n",
    "                    and \"0\" <= sents[sent_idx][span_pos[1] + 1] <= \"9\"\n",
    "                )\n",
    "            ):\n",
    "                continue\n",
    "            drange.append([sent_idx, *span_pos]) # qy: 第几句 从几到几\n",
    "    return drange\n",
    "\n",
    "# qy:将短句子合并为每句总长不超过128\n",
    "def reorganise_sents(sents, max_seq_len, concat=False, final_cut=False, concat_str=\" \"):\n",
    "    new_sents = []\n",
    "    group = \"\"\n",
    "    for sent in sents:\n",
    "        if len(sent) + len(group) < max_seq_len:\n",
    "            if concat:\n",
    "                if len(group) > 1 and \"\\u4e00\" <= group[-1] <= \"\\u9fa5\":\n",
    "                    group += concat_str + sent\n",
    "                else:\n",
    "                    group += sent\n",
    "            else:\n",
    "                new_sents.append(sent)\n",
    "        else:\n",
    "            if len(group) > 0:\n",
    "                new_sents.append(group)\n",
    "                group = \"\"\n",
    "            if len(sent) > max_seq_len:\n",
    "                if final_cut:\n",
    "                    group = sent[:max_seq_len]\n",
    "                else:\n",
    "                    sent_splits = sent_seg(sent, punctuations={\"，\", \"、\", \"|\", \",\"})\n",
    "                    reorg_sent_splits = reorganise_sents(\n",
    "                        sent_splits, max_seq_len, concat=True, final_cut=True\n",
    "                    )\n",
    "                    new_sents.extend(reorg_sent_splits)\n",
    "            else:\n",
    "                group = sent\n",
    "    if len(group) > 0:\n",
    "        new_sents.append(group)\n",
    "    return [s.strip() for s in filter(lambda x: len(x) > 0, new_sents)]\n",
    "\n",
    "\n",
    "def build(\n",
    "    event_type2event_class,\n",
    "    filepath,\n",
    "    dump_filepath,\n",
    "    max_seq_len=128,\n",
    "    inference=False,\n",
    "    add_trigger=False,\n",
    "):\n",
    "    not_valid = 0\n",
    "    data = []\n",
    "    for d in load_line_json_iterator(filepath): # qy:for each document\n",
    "        sents = sent_seg(d[\"text\"], punctuations={\"；\"}) # qy:sentence segmentation\n",
    "        sents = reorganise_sents(sents, max_seq_len, concat=True) # qy:合并短句\n",
    "        # sents = d['map_sentences']\n",
    "        # sentence length filtering\n",
    "        sents = list(filter(lambda x: len(x) >= 5, sents)) # qy:去除<5个字的句子\n",
    "        if(len(d['title'])>0):\n",
    "            sents.insert(0, d[\"title\"])\n",
    "        # sents.insert(0, d['map_title'])\n",
    "        ann_valid_mspans = []\n",
    "        ann_valid_dranges = []\n",
    "        ann_mspan2dranges = defaultdict(list)\n",
    "        ann_mspan2guess_field = {}\n",
    "        recguid_eventname_eventdict_list = [] # qy:event lists\n",
    "\n",
    "        event_types = []\n",
    "        if not inference:\n",
    "            # qy: no events given -> invalid\n",
    "            if \"event_list\" not in d or len(d[\"event_list\"]) == 0:\n",
    "                not_valid += 1\n",
    "                continue\n",
    "\n",
    "            for event_idx, ins in enumerate(d[\"event_list\"]):\n",
    "                event_types.append(ins[\"event_type\"])\n",
    "\n",
    "                roles = event_type2event_class[ins[\"event_type\"]].FIELDS\n",
    "                role2arg = {x: None for x in roles}\n",
    "                # take trigger into consideration\n",
    "                trigger = ins[\"trigger\"]\n",
    "                trigger_ocurr = get_span_drange(sents, trigger)\n",
    "\n",
    "                if len(trigger_ocurr) <= 0:\n",
    "                    continue\n",
    "                if add_trigger:\n",
    "                    role2arg[\"Trigger\"] = trigger\n",
    "                    ann_mspan2guess_field[trigger] = \"Trigger\"\n",
    "                    ann_valid_mspans.append(trigger)\n",
    "                    ann_mspan2dranges[trigger] = trigger_ocurr\n",
    "                for arg_pair in ins[\"arguments\"]:\n",
    "                    ocurr = get_span_drange(sents, arg_pair[\"argument\"])\n",
    "                    if len(ocurr) <= 0:\n",
    "                        continue\n",
    "                    role2arg[arg_pair[\"role\"]] = arg_pair[\"argument\"] # qy: each role only assigned one argument, cover previous ones\n",
    "                    ann_valid_mspans.append(arg_pair[\"argument\"])\n",
    "                    ann_mspan2guess_field[arg_pair[\"argument\"]] = arg_pair[\"role\"]\n",
    "                    ann_mspan2dranges[arg_pair[\"argument\"]] = ocurr\n",
    "                ann_valid_dranges = list(ann_mspan2dranges.values())\n",
    "                recguid_eventname_eventdict_list.append(\n",
    "                    [event_idx, ins[\"event_type\"], role2arg]\n",
    "                )\n",
    "\n",
    "        doc_type = \"unk\"\n",
    "        if len(event_types) > 0:\n",
    "            et_counter = Counter(event_types).most_common()\n",
    "            if len(et_counter) == 1 and et_counter[0][1] == 1:\n",
    "                doc_type = \"o2o\"\n",
    "            elif len(et_counter) == 1 and et_counter[0][1] > 1:\n",
    "                doc_type = \"o2m\"\n",
    "            elif len(et_counter) > 1:\n",
    "                doc_type = \"m2m\"\n",
    "\n",
    "        data.append(\n",
    "            [\n",
    "                d[\"id\"],\n",
    "                {\n",
    "                    \"doc_type\": doc_type,\n",
    "                    \"sentences\": sents,\n",
    "                    \"ann_valid_mspans\": ann_valid_mspans,\n",
    "                    \"ann_valid_dranges\": ann_valid_dranges,\n",
    "                    \"ann_mspan2dranges\": dict(ann_mspan2dranges),\n",
    "                    \"ann_mspan2guess_field\": ann_mspan2guess_field,\n",
    "                    \"recguid_eventname_eventdict_list\": recguid_eventname_eventdict_list,\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    print(\"not valid:\", not_valid)\n",
    "    with open(dump_filepath, \"wt\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(data, fout, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def build_m2m(\n",
    "    event_type2event_class,\n",
    "    filepath,\n",
    "    dump_filepath,\n",
    "    max_seq_len=128,\n",
    "    inference=False,\n",
    "    add_trigger=False,\n",
    "):\n",
    "    not_valid = 0\n",
    "    data = []\n",
    "    for d in load_line_json_iterator(filepath):\n",
    "        sents = sent_seg(d[\"text\"], punctuations={\"；\"})\n",
    "        sents = reorganise_sents(sents, max_seq_len, concat=True)\n",
    "        # sents = d['map_sentences']\n",
    "        # sentence length filtering\n",
    "        sents = list(filter(lambda x: len(x) >= 5, sents))\n",
    "        sents.insert(0, d[\"title\"])\n",
    "        # sents.insert(0, d['map_title'])\n",
    "        ann_valid_mspans = []\n",
    "        ann_valid_dranges = []\n",
    "        ann_mspan2dranges = defaultdict(list)\n",
    "        ann_mspan2guess_field = {}\n",
    "        recguid_eventname_eventdict_list = []\n",
    "\n",
    "        event_types = []\n",
    "        if not inference:\n",
    "            if \"event_list\" not in d or len(d[\"event_list\"]) == 0:\n",
    "                not_valid += 1\n",
    "                continue\n",
    "\n",
    "            for event_idx, ins in enumerate(d[\"event_list\"]):\n",
    "                event_types.append(ins[\"event_type\"])\n",
    "\n",
    "                roles = event_type2event_class[ins[\"event_type\"]].FIELDS\n",
    "                role2arg = {x: [] for x in roles}\n",
    "                # take trigger into consideration\n",
    "                trigger = ins[\"trigger\"]\n",
    "                trigger_ocurr = get_span_drange(sents, trigger)\n",
    "\n",
    "                if len(trigger_ocurr) <= 0:\n",
    "                    continue\n",
    "                if add_trigger:\n",
    "                    role2arg[\"Trigger\"].append(trigger)\n",
    "                    ann_mspan2guess_field[trigger] = \"Trigger\"\n",
    "                    ann_valid_mspans.append(trigger)\n",
    "                    ann_mspan2dranges[trigger] = trigger_ocurr\n",
    "\n",
    "                for arg_pair in ins[\"arguments\"]:\n",
    "                    ocurr = get_span_drange(sents, arg_pair[\"argument\"])\n",
    "                    if len(ocurr) <= 0:\n",
    "                        continue\n",
    "                    role2arg[arg_pair[\"role\"]].append(arg_pair[\"argument\"])\n",
    "                    ann_valid_mspans.append(arg_pair[\"argument\"])\n",
    "                    ann_mspan2guess_field[arg_pair[\"argument\"]] = arg_pair[\"role\"]\n",
    "                    ann_mspan2dranges[arg_pair[\"argument\"]] = ocurr\n",
    "                ann_valid_dranges = list(ann_mspan2dranges.values())\n",
    "                new_role2arg = {x: None for x in roles}\n",
    "                for role, args in role2arg.items():\n",
    "                    if len(args) <= 0:\n",
    "                        new_role2arg[role] = None\n",
    "                    else:\n",
    "                        new_role2arg[role] = args\n",
    "\n",
    "                recguid_eventname_eventdict_list.append(\n",
    "                    [event_idx, ins[\"event_type\"], new_role2arg]\n",
    "                )\n",
    "\n",
    "        et_counter = Counter(event_types).most_common()\n",
    "        if len(et_counter) == 1 and et_counter[0][1] == 1:\n",
    "            doc_type = \"o2o\"\n",
    "        elif len(et_counter) == 1 and et_counter[0][1] > 1:\n",
    "            doc_type = \"o2m\"\n",
    "        elif len(et_counter) > 0:\n",
    "            doc_type = \"m2m\"\n",
    "        else:\n",
    "            doc_type = \"unk\"\n",
    "\n",
    "        data.append(\n",
    "            [\n",
    "                d[\"id\"],\n",
    "                {\n",
    "                    \"doc_type\": doc_type,\n",
    "                    \"sentences\": sents,\n",
    "                    \"ann_valid_mspans\": ann_valid_mspans,\n",
    "                    \"ann_valid_dranges\": ann_valid_dranges,\n",
    "                    \"ann_mspan2dranges\": dict(ann_mspan2dranges),\n",
    "                    \"ann_mspan2guess_field\": ann_mspan2guess_field,\n",
    "                    \"recguid_eventname_eventdict_list\": recguid_eventname_eventdict_list,\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    print(\"not valid:\", not_valid)\n",
    "    with open(dump_filepath, \"wt\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(data, fout, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def stat_roles(filepath):\n",
    "    type2roles = defaultdict(set)\n",
    "    for d in load_line_json_iterator(filepath):\n",
    "        if \"event_list\" not in d:\n",
    "            continue\n",
    "        for event_idx, ins in enumerate(d[\"event_list\"]):\n",
    "            for arg_pair in ins[\"arguments\"]:\n",
    "                type2roles[ins[\"event_type\"]].add(arg_pair[\"role\"])\n",
    "\n",
    "    for event_type in type2roles:\n",
    "        print(event_type, len(type2roles[event_type]), list(type2roles[event_type]))\n",
    "\n",
    "\n",
    "def merge_pred_ents_to_inference(pred_filepath, inference_filepath, dump_filepath):\n",
    "    inference_data = load_json(inference_filepath)\n",
    "    pred_data = {}\n",
    "    pred_sents = {}\n",
    "    pred_titles = {}\n",
    "    for pred in load_line_json_iterator(pred_filepath):\n",
    "        pred_data[pred[\"id\"]] = pred[\"entity_pred\"]\n",
    "        pred_sents[pred[\"id\"]] = pred[\"map_sentences\"]\n",
    "        pred_titles[pred[\"id\"]] = pred[\"map_title\"]\n",
    "    for d in inference_data:\n",
    "        guid = d[0]\n",
    "        d[1][\"sentences\"] = pred_sents[guid]\n",
    "        d[1][\"sentences\"].insert(0, pred_titles[guid])\n",
    "        epd = pred_data[guid]\n",
    "        ann_valid_mspans = []\n",
    "        ann_valid_dranges = []\n",
    "        ann_mspan2guess_field = {}\n",
    "        ann_mspan2dranges = defaultdict(list)\n",
    "        for ent in epd:\n",
    "            if \"trigger\" in ent[1].lower():\n",
    "                # ent_type = 'Trigger'\n",
    "                continue\n",
    "            else:\n",
    "                ent_type = ent[1].split(\"-\")[-1]\n",
    "            ann_mspan2guess_field[ent[0]] = ent_type\n",
    "            ann_mspan2dranges[ent[0]].append([ent[2] + 1, ent[3], ent[4] + 1])\n",
    "        # for ent, ent_type in ent_pairs:\n",
    "        #     drange = get_span_drange(d[1]['sentences'], ent)\n",
    "        #     if len(drange) == 0:\n",
    "        #         continue\n",
    "        #     ann_mspan2guess_field[ent] = ent_type\n",
    "        #     ann_mspan2dranges[ent] = drange\n",
    "        ann_mspan2dranges = dict(ann_mspan2dranges)\n",
    "        ann_valid_mspans = list(ann_mspan2dranges.keys())\n",
    "        ann_valid_dranges = list(ann_mspan2dranges.values())\n",
    "        d[1][\"ann_valid_mspans\"] = ann_valid_mspans\n",
    "        d[1][\"ann_valid_dranges\"] = ann_valid_dranges\n",
    "        d[1][\"ann_mspan2guess_field\"] = ann_mspan2guess_field\n",
    "        d[1][\"ann_mspan2dranges\"] = ann_mspan2dranges\n",
    "\n",
    "    with open(dump_filepath, \"wt\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(inference_data, fout, ensure_ascii=False)\n",
    "\n",
    "    print(json.dumps(inference_data[:2], ensure_ascii=False, indent=2))\n",
    "\n",
    "\n",
    "def merge_pred_ents_with_pred_format_to_inference(\n",
    "    pred_filepath, inference_filepath, dump_filepath\n",
    "):\n",
    "    inference_data = load_json(inference_filepath)\n",
    "    pred_data = {}\n",
    "    for pred in load_line_json_iterator(pred_filepath):\n",
    "        pred_data[pred[\"id\"]] = pred[\"new_comments\"]\n",
    "    for d in inference_data:\n",
    "        guid = d[0]\n",
    "        d[1][\"sentences\"] = pred_data[guid][\"sentences\"]\n",
    "        ann_valid_mspans = []\n",
    "        ann_valid_dranges = []\n",
    "        ann_mspan2guess_field = {}\n",
    "        ann_mspan2dranges = defaultdict(list)\n",
    "        for ent in pred_data[guid][\"mspans\"]:\n",
    "            if \"trigger\" in ent[\"mtype\"].lower():\n",
    "                # ent_type = 'Trigger'\n",
    "                continue\n",
    "            else:\n",
    "                ent_type = ent[\"mtype\"].split(\"-\")[-1]\n",
    "            ann_mspan2guess_field[ent[\"msapn\"]] = ent_type\n",
    "            ann_mspan2dranges[ent[\"msapn\"]].append(ent[\"drange\"])\n",
    "        ann_mspan2dranges = dict(ann_mspan2dranges)\n",
    "        ann_valid_mspans = list(ann_mspan2dranges.keys())\n",
    "        ann_valid_dranges = list(ann_mspan2dranges.values())\n",
    "        d[1][\"ann_valid_mspans\"] = ann_valid_mspans\n",
    "        d[1][\"ann_valid_dranges\"] = ann_valid_dranges\n",
    "        d[1][\"ann_mspan2guess_field\"] = ann_mspan2guess_field\n",
    "        d[1][\"ann_mspan2dranges\"] = ann_mspan2dranges\n",
    "\n",
    "    with open(dump_filepath, \"wt\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(inference_data, fout, ensure_ascii=False)\n",
    "\n",
    "    print(json.dumps(inference_data[:2], ensure_ascii=False, indent=2))\n",
    "\n",
    "\n",
    "def multi_role_stat(filepath):\n",
    "    num_ins = 0\n",
    "    num_multi_role_doc = 0\n",
    "    type2num_multi_role = defaultdict(lambda: 0)\n",
    "    type2role2num_multi_role = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for d in load_line_json_iterator(filepath):\n",
    "        if \"event_list\" not in d:\n",
    "            continue\n",
    "        for ins in d[\"event_list\"]:\n",
    "            num_ins += 1\n",
    "            roles = [x[\"role\"] for x in ins[\"arguments\"]]\n",
    "            role, role_cnt = Counter(roles).most_common(1)[0]\n",
    "            if role_cnt > 1:\n",
    "                # if ins['event_type'] == '高管变动' and role == '高管职位':\n",
    "                #     breakpoint()\n",
    "                num_multi_role_doc += 1\n",
    "                type2num_multi_role[ins[\"event_type\"]] += 1\n",
    "                type2role2num_multi_role[ins[\"event_type\"]][role].append(role_cnt)\n",
    "\n",
    "    print(\"num_ins\", num_ins)\n",
    "    print(\"num_multi_role_doc\", num_multi_role_doc)\n",
    "    print(\"type2num_multi_role\", type2num_multi_role)\n",
    "    for event_type in type2role2num_multi_role:\n",
    "        for role in type2role2num_multi_role[event_type]:\n",
    "            # type2role2num_multi_role[event_type][role] = Counter(type2role2num_multi_role[event_type][role]).most_common()\n",
    "            type2role2num_multi_role[event_type][role] = sum(\n",
    "                type2role2num_multi_role[event_type][role]\n",
    "            )\n",
    "    print(\"type2role2num_multi_role\", type2role2num_multi_role)\n",
    "\n",
    "\n",
    "def stat_shared_triggers(filepath):\n",
    "    # train: 3400 / 9498\n",
    "    num_records = 0\n",
    "    num_share_trigger_records = 0\n",
    "    with open(filepath, \"rt\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            trigger2event = defaultdict(list)\n",
    "            data = json.loads(line)\n",
    "            for ins in data.get(\"event_list\", []):\n",
    "                num_records += 1\n",
    "                trigger2event[ins[\"trigger\"]].append(ins)\n",
    "            for trigger, inses in trigger2event.items():\n",
    "                if len(inses) > 1:\n",
    "                    num_share_trigger_records += len(inses)\n",
    "    print(\n",
    "        f\"num_records: {num_records}, num_share_trigger_records: {num_share_trigger_records}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEvent(object):\n",
    "    def __init__(self, fields, event_name='Event', key_fields=(), recguid=None):\n",
    "        self.recguid = recguid\n",
    "        self.name = event_name\n",
    "        self.fields = list(fields)\n",
    "        self.field2content = {f: None for f in fields}\n",
    "        self.nonempty_count = 0\n",
    "        self.nonempty_ratio = self.nonempty_count / len(self.fields)\n",
    "\n",
    "        self.key_fields = set(key_fields)\n",
    "        for key_field in self.key_fields:\n",
    "            assert key_field in self.field2content\n",
    "\n",
    "    def __repr__(self):\n",
    "        event_str = \"\\n{}[\\n\".format(self.name)\n",
    "        event_str += \"  {}={}\\n\".format(\"recguid\", self.recguid)\n",
    "        event_str += \"  {}={}\\n\".format(\"nonempty_count\", self.nonempty_count)\n",
    "        event_str += \"  {}={:.3f}\\n\".format(\"nonempty_ratio\", self.nonempty_ratio)\n",
    "        event_str += \"] (\\n\"\n",
    "        for field in self.fields:\n",
    "            if field in self.key_fields:\n",
    "                key_str = \" (key)\"\n",
    "            else:\n",
    "                key_str = \"\"\n",
    "            event_str += \"  \" + field + \"=\" + str(self.field2content[field]) + \", {}\\n\".format(key_str)\n",
    "        event_str += \")\\n\"\n",
    "        return event_str\n",
    "\n",
    "    def update_by_dict(self, field2text, recguid=None):\n",
    "        self.nonempty_count = 0\n",
    "        self.recguid = recguid\n",
    "\n",
    "        for field in self.fields:\n",
    "            if field in field2text and field2text[field] is not None:\n",
    "                self.nonempty_count += 1\n",
    "                self.field2content[field] = field2text[field]\n",
    "            else:\n",
    "                self.field2content[field] = None\n",
    "\n",
    "        self.nonempty_ratio = self.nonempty_count / len(self.fields)\n",
    "\n",
    "    def field_to_dict(self):\n",
    "        return dict(self.field2content)\n",
    "\n",
    "    def set_key_fields(self, key_fields):\n",
    "        self.key_fields = set(key_fields)\n",
    "\n",
    "    def is_key_complete(self):\n",
    "        for key_field in self.key_fields:\n",
    "            if self.field2content[key_field] is None:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def get_argument_tuple(self):\n",
    "        args_tuple = tuple(self.field2content[field] for field in self.fields)\n",
    "        return args_tuple\n",
    "\n",
    "    def is_good_candidate(self, min_match_count=2):\n",
    "        key_flag = self.is_key_complete()\n",
    "        if key_flag:\n",
    "            if self.nonempty_count >= min_match_count:\n",
    "                return True\n",
    "        return False\n",
    "class event_0(BaseEvent):\n",
    "\tNAME = '股票事件'\n",
    "\tFIELDS   = ['股票代码', '股票名称', '股票评级', '评级变化']\n",
    "\tTRIGGERS = {1: ['股票代码'],\n",
    "   2: ['股票名称','评级变化'],\n",
    "   3: ['股票代码','股票名称','评级变化'],\n",
    "   4: ['股票代码','股票名称','股票评级','评级变化']}\n",
    "\n",
    "\tTRIGGERS['all'] = ['股票代码', '股票名称', '股票评级', '评级变化']\n",
    "\tdef __init__(self, recguid=None):\n",
    "\t\tsuper().__init__(\n",
    "\t\tself.FIELDS, event_name=self.NAME, recguid=recguid\n",
    "\t\t )\n",
    "\t\tself.set_key_fields(self.TRIGGERS)\n",
    "\n",
    "\n",
    "\n",
    "event_type2event_class = { event_0.NAME: event_0,}\n",
    "event_type_fields_list = [(event_0.NAME,event_0.FIELDS,event_0.TRIGGERS,2),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'submit_test.json'\n",
    "#dump_filepath,\n",
    "max_seq_len=128\n",
    "inference=False\n",
    "add_trigger=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中信建投证券CHINASECURITIES证券研究报告·A股公司简评报告中药II持续完善品种布局,健康产业引领者起航华润三九(000999)事件维持买入公司拟收购澳诺中国100%股权及引入抗肿瘤创新药物11月27日晚,', '公司公告:1)拟收购誉衡药业持有的澳诺(中国)制药有限公司(以下简称\"澳诺制药\"或\"目标公司\")100%股权;2)拟购买沈阳药科大学所持有的QBH-196项目所有技术成果及知识产权,以及在相关专利所涉及的有关国家和地区开发经营该产品的权利。', '简评拟收购澳诺制药,补充儿科产品线公司拟拟收购誉衡药业持有的澳诺制药100%股权,交易价款共计人民币14.2亿元,资金来源为公司自有资金。', '澳诺制药是集生产、研发、销售于一体的高新技术企业,核心产品为葡萄糖酸钙锌口服溶液、维生素C咀嚼片、参芝石斜颗粒,其\"澳诺\"、\"金辛金丐特\"是儿童补钙知名品牌。\"澳诺\"、\"金辛金丐特\"牌葡萄糖酸钙锌口服溶液是儿童补钙大产品,具备良好的市场规模和成长性。', '近年来,该产品及品牌连续位列零售市场钙补充剂第二位,钙补充剂药品市场第一位。零售渠道及医疗渠道市场份额均持续增长。', '自我诊疗是重要战略方向之一,儿童健康是自我诊疗业务重点发|我们认为,公司致力于成为\"大众医药健康产业的引领者\",|展的领域,葡萄糖酸钙锌口服溶液是儿童补钙大品种,具备良好预测和比率2018|2019E|2020E|2021E|营业收入(百万)|13,', '427.7|15,098.2|16,831.4|18,767.0|贺菊颖hejuying@csc.com.cn010-86451162执业证书编号:|S1440517050001|', '刘若飞liuruofei@csc.com.cn010-85130388执业证书编号:|S1440519080003|发布日期:|2019年11月28日|当前股价:|29.38元|主要数据股票价格绝对/相对市场表现(%|(%)|1个月|3个月|12个月|', '-9.04/-7.29|-4.55/-5.93|22.31/9.6|12月最高/最低价(元)|33.6/21.93|总股本(万股)|97,890.0|流通A股(万股)|97,839.53|总市值(亿元)|287.6|流通市值(亿元)|287.45|', '近3月日均成交量(万)|524.31|主要股东华润医药控股有限公司|63.6%|营业收入增长率EBITDA(百万)EBITDA增长率净利润(百万)净利润增长率ROEEPS(元)P/E20.8%2,227.833.5%1,', '432.110.0%13.2%1.4620.512.4%2,964.333.1%2,277.659.0%18.1%2.3312.911.5%2,698.8-9.0%2,044.1-10.3%14.6%2.0914.311.5%3,009.811.5%2,', '327.013.8%14.8%2.3812.6股价表现51%31%11%-9%I|I|I|I|i|华润三九|上证指数|2019/2/27|2019/5/27|2019/6/27|.2019/7/27|2019/8/27|类2019/9/27|', '2019/10/27|2019/11/27|P/BEV/EBITDA请参阅最后一页的重要声明2.712.02.37.62.19.71.96.3相关研究报告【中信建投中药II】华润三九(000999):19:08:23|持续推进品牌建设,品牌中药龙头强者|', '恒强18:03:16|诊疗领航者起航|华润三九(000999):业绩稳健,自我|']\n",
      "109\n",
      "120\n",
      "69\n",
      "125\n",
      "57\n",
      "125\n",
      "90\n",
      "126\n",
      "124\n",
      "110\n",
      "127\n",
      "121\n",
      "127\n",
      "40\n",
      "len:14\n"
     ]
    }
   ],
   "source": [
    "not_valid = 0\n",
    "data = []\n",
    "\n",
    "for d in load_line_json_iterator(filepath): # qy:for each document\n",
    "    sents = sent_seg(d[\"text\"], punctuations={\"；\"})\n",
    "    sents = reorganise_sents(sents, max_seq_len, concat=True) # qy:合并短句\n",
    "    #print(d['id'])\n",
    "    if(d['id']==269):\n",
    "        print(sents)\n",
    "        for s in sents:\n",
    "            print(len(s))\n",
    "        print(\"len:\"+str(len(sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
